{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oskar tracker\n",
    "- Author: Savandara BESSE & Leo Blondel\n",
    "- Creation: 10-05-2019\n",
    "- Last modification: 04-16-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lblondel/.local/lib/python3.5/site-packages/Bio/SearchIO/__init__.py:211: BiopythonExperimentalWarning: Bio.SearchIO is an experimental submodule which may undergo significant changes prior to its future official release.\n",
      "  BiopythonExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, progressbar\n",
    "from Bio import SeqIO\n",
    "from Bio import SearchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.A : Create protein sequences from TSA data\n",
    "\n",
    "### Required inputs\n",
    "- TSA RAW folder: `/PATH/TO/TSA_INPUTS`\n",
    "- TSA Protein folder : `PATH/TO/PROCESSED/TSA`\n",
    "\n",
    "### Main steps:\n",
    "1. Unzip TSA files\n",
    "2. Change the exension fsa_nt to fasta\n",
    "3. Translation step (with clean translation end)\n",
    "4. Move translated TSA to TSA output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TSA_inputs(TSA_path, protein_path):\n",
    "    protein_list = []\n",
    "    bar = progressbar.ProgressBar()\n",
    "    if not os.path.isdir(protein_path):\n",
    "        os.mkdir(protein_path)\n",
    "        \n",
    "#     os.system('gunzip {}/*'.format(TSA_path))\n",
    "#     os.system('rename \\'s/fsa_nt$/fasta/\\' {}/*.fsa_nt'.format(TSA_path))\n",
    "    \n",
    "    for TSA in bar(os.listdir(TSA_path)):\n",
    "        INPUT = os.path.join(TSA_path, TSA)\n",
    "        OUTPUT = os.path.join(protein_path, 'translated_{}'.format(TSA))\n",
    "        os.system('transeq -sequence {} -frame 6 -trim -clean -outseq {}'.format(INPUT,OUTPUT))    \n",
    "        protein_list.append(OUTPUT)\n",
    "    return protein_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run these steps you need to provide the folder path of your raw TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insect analysis 2019 \n",
    "# %%time \n",
    "# TSA_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_INPUTS'\n",
    "# protein_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_PROTEIN'\n",
    "# translated_TSA = build_TSA_inputs(TSA_path, protein_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crustacean analysis 2017\n",
    "# %%time \n",
    "# TSA_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_INPUTS'\n",
    "# protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_PROTEIN'\n",
    "# translated_TSA = build_TSA_inputs(TSA_path, protein_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.B: Collect protein sequences from GCF data\n",
    "\n",
    "### Required inputs\n",
    "- Genome folder: `/PATH/TO/GENOMES`\n",
    "- GCF Protein folder: `PATH/TO/PROCESSED/GCF`\n",
    "\n",
    "\n",
    "### Main steps:\n",
    "1. Copy GCF protein fasta files to GCF output folder\n",
    "2. Unzip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_Proteins(currentFolder):\n",
    "    for gFile in os.listdir(currentFolder):\n",
    "        if '_protein.faa.gz' in gFile :\n",
    "            return gFile\n",
    "        \n",
    "def build_GCF_inputs(genome_path, res_path):\n",
    "    for folder in os.listdir(genome_path): \n",
    "        if 'GCF' in folder :\n",
    "            currentFolder = os.path.join(genome_path, folder)\n",
    "            proteome = reach_Proteins(currentFolder)\n",
    "            target = os.path.join(genome_path, currentFolder, proteome)\n",
    "            os.system('cp {} {}'.format(target, res_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run these steps you need to provide the folder path of your Genome folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# genome_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/Genomes'\n",
    "# protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_PROTEIN'\n",
    "# build_GCF_inputs(genome_path, protein_path)\n",
    "# os.system('gunzip {}/*'.format(protein_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.C: Collect protein sequences from GCA data\n",
    "\n",
    "### Required inputs\n",
    "- Genome folder: '/PATH/TO/GENOMES' \n",
    "- GCA Protein folder: 'PATH/TO/PROCESSED/GCA'\n",
    "\n",
    "\n",
    "### Main steps:\n",
    "/!\\ Need to complete Augustus annotations of non annotated genomes by running 1.3_run_augustus_training.py\n",
    "1. `reach_Genes()` will collect specifically genome fasta file for a given genome assembly\n",
    "2. `build_GCA_inputs()` will generate the input folders needed for 1.4_run_augustus.py \n",
    "/!\\ Run 1.4_run_augustus.py to generate sbatch scripts, wait until completion of all the annotations\n",
    "3. Extraction of protein sequences from annotated genomes (GFF files)\n",
    "    - Run getAnnoFasta.pl available from `scripts` folder of Augustus code source\n",
    "3. Copy GCA protein fasta files to GCA output folder\n",
    "4. Add Augustus model used for each non-annotated genomes registered in `../Data/01_Oskar_identification/2019/genome_insect_database.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_Genes(currentFolder):\n",
    "    all_genomic_files = [ gFile for gFile in os.listdir(currentFolder) if '_genomic.fna.gz' in gFile ]\n",
    "    if len(all_genomic_files) > 1 :\n",
    "        for gFile in all_genomic_files :\n",
    "            if ('cds' in gFile) or ('rna' in gFile):\n",
    "                pass\n",
    "            else :\n",
    "                return gFile\n",
    "    else :\n",
    "        return all_genomic_files[0]\n",
    "        \n",
    "def build_GCA_inputs(genome_path, res_path):\n",
    "    for folder in os.listdir(genome_path): \n",
    "        if 'GCA' in folder :\n",
    "            currentFolder = os.path.join(genome_path, folder)\n",
    "            proteome = reach_Genes(currentFolder)\n",
    "            target = os.path.join(genome_path, currentFolder, proteome)\n",
    "            os.system('cp {} {}'.format(target, res_path))\n",
    "\n",
    "def set_augustus_model(order, table):\n",
    "    if order == 'Diplura' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Archaeognatha':\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Odonata' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Ephemeroptera' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Plecoptera' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Orthoptera' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Phasmatodea' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Blattodea':\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Thysanoptera' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Hemiptera' :\n",
    "        hmm_order = 'bemisia_tabaci'\n",
    "    elif order == 'Phthiraptera' :\n",
    "        hmm_order = 'pediculus_humanus'\n",
    "    elif order == 'Hymenoptera' :\n",
    "        hmm_order = 'apis_mellifera'\n",
    "    elif order == 'Strepsiptera' :\n",
    "        hmm_order = 'tribolium_castaneum'\n",
    "    elif order == 'Coleoptera' :\n",
    "        hmm_order = 'tribolium_castaneum'\n",
    "    elif order == 'Trichoptera' :\n",
    "        hmm_order = 'papilio_xuthus'\n",
    "    elif order == 'Lepidoptera' :\n",
    "        hmm_order = 'papilio_xuthus'\n",
    "    elif order == 'Siphonaptera' :\n",
    "        hmm_order = 'ctenophalides_felis'\n",
    "    elif order == 'Diptera' :\n",
    "        family = table[table['order_name'] == order ]['family_name']\n",
    "        if 'Culicidae' in family :\n",
    "            hmm_order = 'aedes'\n",
    "        elif 'Pteromalidae' in family :\n",
    "            hmm_order = 'nasonia'\n",
    "        else :\n",
    "            hmm_order = 'fly'\n",
    "    return hmm_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# genome_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/Genomes'\n",
    "# res_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_INPUTS'\n",
    "# build_GCA_inputs(genome_path, res_path)\n",
    "# os.system('gunzip {}/*'.format(res_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCA = GCA[GCA['genome_id'].str.contains('GCA')]\n",
    "GCA['pgc_mode'] = GCA['order_name'].apply(set_germ_cell_formation)\n",
    "GCA['augustus_model'] = GCA['order_name'].apply(set_augustus_model, args=(GCA,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Track Oskar\n",
    "\n",
    "### Required inputs:\n",
    "- LOTUS pHMM (version 3)\n",
    "- OSK pHMM (version 3)\n",
    "- TSA / GCF / GCA protein folders\n",
    "\n",
    "### Main steps: \n",
    "- Run `execute_hmmsearch.py` for each type of data\n",
    "- Copy the generated results in `../Data/01_Oskar_identification/hmmsearch_raw_results/XXX_V3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/storage/Oskar_Evolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "## Crustacean analysis \n",
    "lotus_model = '../Data/Oskar_hmm/V3/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/V3/OSK_CONSENSUS.hmm'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_PROTEIN'\n",
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_RESULT'\n",
    "os.system('python3 execute_hmmsearch.py -a TSA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "## Insect analysis\n",
    "lotus_model = '../Data/Oskar_hmm/V3/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/V3/OSK_CONSENSUS.hmm'\n",
    "protein_path = os.path.join(storage_path, 'TSA_PROTEIN')\n",
    "result_path =  os.path.join(storage_path, 'TSA_RESULTS')\n",
    "os.system('python3 execute_hmmsearch.py -a TSA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {os.path.join(storage_path, 'TSA_RESULTS', '*')} ../Data/01_Oskar_identification/hmmsearch_raw_results/TSA_V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "lotus_model = '../Data/Oskar_hmm/V3/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/V3/OSK_CONSENSUS.hmm'\n",
    "protein_path =  os.path.join(storage_path, 'GCF_PROTEIN')\n",
    "result_path = os.path.join(storage_path, 'GCF_RESULTS')\n",
    "os.system('python3 execute_hmmsearch.py -a GCF -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {os.path.join(storage_path, 'GCF_RESULTS', '*')} ../Data/01_Oskar_identification/hmmsearch_raw_results/GCF_V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.23 ms, sys: 4.08 ms, total: 9.31 ms\n",
      "Wall time: 55.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "lotus_model = '../Data/Oskar_hmm/V3/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/V3/OSK_CONSENSUS.hmm'\n",
    "protein_path =  os.path.join(storage_path, 'GCA_PROTEIN')\n",
    "result_path = os.path.join(storage_path, 'GCA_RESULTS')\n",
    "os.system('python3 execute_hmmsearch.py -a GCA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {os.path.join(storage_path, 'GCA_RESULTS', '*')} ../Data/01_Oskar_identification/hmmsearch_raw_results/GCA_V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Collect Oskar candidates\n",
    "\n",
    "### Required inputs\n",
    "- TSA / GCF / GCA Oskar result folders  `../Data/01_Oskar_identification/hmmsearch_raw_results/XXX_V3`\n",
    "\n",
    "### Main steps\n",
    "1. Identification of Oskar candidates by getting LOTUS and OSK hits\n",
    "2. Add information of PGC mode\n",
    "3. Save Oskar candidates in tables per data type available in `../Data/01_Oskar_identification/oskar_tracker_results/XXX_V3/xxx_oskar_results.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_Table(TSA, HMM):\n",
    "    for DOM in HMM :\n",
    "        if TSA in DOM :\n",
    "            return DOM\n",
    "        \n",
    "def collect_Hits(hmmerTable):\n",
    "    f = open(hmmerTable)\n",
    "    tmp = [ line for line in f.readlines() if '#' not in line ]\n",
    "    f.close()\n",
    "    if len(tmp) != 0 : \n",
    "        return [ hit.id for hit in SearchIO.read(hmmerTable, 'hmmer3-tab') if hit.evalue <= 0.05 ]\n",
    "    return []\n",
    "    \n",
    "\n",
    "def oskar_analysis(ID):\n",
    "    lotus_res = os.path.join(result_path, collect_Table(ID, LOTUS))\n",
    "    osk_res = os.path.join(result_path, collect_Table(ID, OSK))\n",
    "\n",
    "    lotus_hits = collect_Hits(lotus_res)\n",
    "    osk_hits = collect_Hits(osk_res)\n",
    "    oskar_hits = list(set(lotus_hits) & set(osk_hits))\n",
    "    \n",
    "    if len(oskar_hits) == 0 :\n",
    "        oskar_hits = 'None'\n",
    "    else : \n",
    "        oskar_hits = ','.join(list(set(lotus_hits) & set(osk_hits)))\n",
    "    if len(lotus_hits) == 0 :\n",
    "        lotus_hits = 'None'\n",
    "    else :\n",
    "        lotus_hits = ','.join(list(lotus_hits))\n",
    "    if len(osk_hits) == 0:\n",
    "        osk_hits = 'None'\n",
    "    else : \n",
    "        osk_hits = ','.join(list(osk_hits))\n",
    "\n",
    "    return [ID, lotus_hits, osk_hits, oskar_hits ]\n",
    "        \n",
    "PGC_mode = {\n",
    "    'Collembola':'Induction',\n",
    "    'Diplura':'Induction',\n",
    "    'Archaeognatha':'Induction',\n",
    "    'Zygentoma':'Induction',\n",
    "    'Odonata':'Induction',\n",
    "    'Ephemeroptera':'Induction',\n",
    "    'Zoraptera':'Induction',\n",
    "    'Dermaptera':'Induction',\n",
    "    'Plecoptera':'Induction',\n",
    "    'Orthoptera':'Induction',\n",
    "    'Mantophasmatodea':'Induction',\n",
    "    'Grylloblattodea':'Induction',\n",
    "    'Embioptera':'Induction',\n",
    "    'Phasmatodea':'Induction',\n",
    "    'Mantodea':'Induction',\n",
    "    'Blattodea':'Induction',\n",
    "    'Isoptera':'Induction',\n",
    "    'Thysanoptera':'Induction',\n",
    "    'Hemiptera':'Induction',\n",
    "    'Phthiraptera':'Induction',\n",
    "    'Psocoptera':'Induction',\n",
    "    'Hymenoptera':'Inheritance',\n",
    "    'Raphidioptera':'Inheritance',\n",
    "    'Megaloptera':'Inheritance',\n",
    "    'Neuroptera':'Inheritance',\n",
    "    'Strepsiptera':'Inheritance',\n",
    "    'Coleoptera':'Inheritance',\n",
    "    'Trichoptera':'Inheritance',\n",
    "    'Lepidoptera':'Inheritance',\n",
    "    'Siphonaptera':'Inheritance',\n",
    "    'Mecoptera':'Inheritance',\n",
    "    'Diptera':'Inheritance'\n",
    "}\n",
    "\n",
    "def set_germ_cell_formation(x):\n",
    "    return PGC_mode[x]\n",
    "\n",
    "def formatTSA(x):\n",
    "    x = x.replace('.1', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Crustacean analysis \n",
    "tsa_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/TSA_crustacea'\n",
    "IDs = list(set([ TSA.split('_')[0] for TSA in os.listdir(tsa_path) ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(tsa_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(tsa_path)) if 'osk' in domain ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSA_OSKAR = list(map(oskar_analysis, IDs))\n",
    "TMP = pd.DataFrame(TSA_OSKAR, columns=['TSA', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "TMP['TSA'] = TMP['TSA'].apply(formatTSA)\n",
    "TSA = pd.read_csv('../Data/01_Oskar_identification/2017/transcriptome_crustacean_database.csv')\n",
    "TSA = TSA.merge(TMP, on='TSA')\n",
    "TSA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA_crustacea/tsa_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 0 ns, total: 1.19 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/TSA_V3'\n",
    "IDs = list(set([ TSA.split('_')[0] for TSA in os.listdir(result_path) ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(result_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(result_path)) if 'osk' in domain ]\n",
    "TSA_OSKAR = list(map(oskar_analysis, IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP = pd.DataFrame(TSA_OSKAR, columns=['tsa_abrv', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "TSA = pd.read_csv('../Data/01_Oskar_identification/2019/transcriptome_insect_database.csv')\n",
    "\n",
    "TSA['pgc_mode'] = TSA['order_name'].apply(set_germ_cell_formation)\n",
    "TSA['tsa_abrv'] = [ '{}{}'.format(TSA['tsa_id'][i][:5], TSA['tsa_id'][i].split('.')[1]) for i in range(len(TSA)) ]\n",
    "\n",
    "TSA = TSA.merge(TMP, on='tsa_abrv')\n",
    "TSA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/GCF_V3'\n",
    "IDs = list(set([ '{}_{}'.format(GCF.split('_')[0], GCF.split('_')[1]) for GCF in os.listdir(result_path) ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(result_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(result_path)) if 'osk' in domain ]\n",
    "GCF_OSKAR = list(map(oskar_analysis, IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP = pd.DataFrame(GCF_OSKAR, columns=['genome_id', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "\n",
    "GCF = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCF['pgc_mode'] = GCF['order_name'].apply(set_germ_cell_formation)\n",
    "\n",
    "GCF = GCF.merge(TMP, on='genome_id')\n",
    "GCF.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/GCA_V3'\n",
    "IDs = list(set([ '{}_{}'.format(GCA.split('_')[0], GCA.split('_')[1]) for GCA in os.listdir(result_path) ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(result_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(result_path)) if 'osk' in domain ]\n",
    "GCA_OSKAR = list(map(oskar_analysis, IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP = pd.DataFrame(GCA_OSKAR, columns=['genome_id', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "\n",
    "GCA = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCA['pgc_mode'] = GCA['order_name'].apply(set_germ_cell_formation)\n",
    "GCA['augustus_model'] = GCA['order_name'].apply(set_augustus_model, args=(GCA,))\n",
    "\n",
    "GCA = GCA.merge(TMP, on='genome_id')\n",
    "GCA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save Oskar candidates\n",
    "### Required inputs\n",
    "- TSA / GCF / GCA protein folders\n",
    "- Names of FASTA output files `../Data/01_Oskar_identification/oskar_tracker_results/XXX_V3/xxx_oskar_candidates.fasta`\n",
    "\n",
    "### Main steps\n",
    "1. Save Oskar candidates in different fasta files according data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_genome(x, table):\n",
    "    return table[table['oskar_hits'].str.contains(x)]['genome_id'].values[0]\n",
    "\n",
    "def retrieveSeq(seqID, dataType, fastaFile):\n",
    "    for seqRecord in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        if seqID in seqRecord.id :\n",
    "            if 'TSA' in dataType :\n",
    "                seqRecord.seq = format_Seq(seqRecord.seq)\n",
    "            return seqRecord\n",
    "        \n",
    "def first_AA(seq):\n",
    "    for i in range(len(seq)):\n",
    "        if 'M' in seq[i]:\n",
    "            return i\n",
    "\n",
    "def format_Seq(seq):\n",
    "    return seq[first_AA(seq):]\n",
    "\n",
    "def save_oskar(TABLE, protein_path, dataType, result_file):\n",
    "    OSKAR_SEQ = [] \n",
    "    OSKAR_HITS = [ ID for LIST in TABLE[TABLE['oskar_hits'] != 'None']['oskar_hits'] for ID in LIST.split(',') ]\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for SEQ in bar(OSKAR_HITS): \n",
    "        if 'TSA' in dataType :\n",
    "            ID = SEQ[:4]\n",
    "        else :\n",
    "            ID = collect_genome(SEQ, TABLE)\n",
    "        for PROTEOME in os.listdir(protein_path):\n",
    "            if ID in PROTEOME :\n",
    "                FASTA = os.path.join(protein_path, PROTEOME)\n",
    "                OSKAR_SEQ.append(retrieveSeq(SEQ, dataType, FASTA))\n",
    "    SeqIO.write(OSKAR_SEQ, result_file, 'fasta')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_candidates.fasta'\n",
    "save_oskar(TSA, protein_path, 'TSA', result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (98 of 98) |########################| Elapsed Time: 0:00:09 Time:  0:00:09\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_candidates.fasta'\n",
    "save_oskar(GCF, protein_path, 'GCF', result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_candidates.fasta'\n",
    "save_oskar(GCA, protein_path, 'GCA', result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Oskar hmmsearch files for crossed-validation \n",
    "\n",
    "### Required inputs\n",
    "- OSKAR pHMM (version 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsa_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/V3/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/final_tsa_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, tsa_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcf_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/V3/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/final_gcf_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, gcf_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gca_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/V3/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/final_gca_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, gca_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Oskar duplicate and isoform detection and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from Bio import SeqIO\n",
    "from Bio import SearchIO\n",
    "from Bio import Align\n",
    "from Bio import AlignIO\n",
    "from Bio import Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSA_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_results.csv')\n",
    "TSA_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_candidates.fasta'\n",
    "TSA_hmmer = SearchIO.read('../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/final_tsa_oskar_search.txt', 'hmmer3-tab')\n",
    "TSA_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCF_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_results.csv')\n",
    "GCF_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_candidates.fasta'\n",
    "GCF_hmmer = SearchIO.read('../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/final_gcf_oskar_search.txt', 'hmmer3-tab')\n",
    "GCF_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_results.csv')\n",
    "GCA_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_candidates.fasta'\n",
    "GCA_hmmer = SearchIO.read('../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/final_gca_oskar_search.txt', 'hmmer3-tab')\n",
    "GCA_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Hamming distance between string1 and string2.\n",
    "# string1 and string2 should be the same length.\n",
    "def hamming_distance(seq1, seq2): \n",
    "    assert(len(seq1) == len(seq2)) \n",
    "    # Start with a distance of zero, and count up\n",
    "    distance = 0.0\n",
    "    # Loop over the indices of the string\n",
    "    L = len(seq1)\n",
    "    for i in range(L):\n",
    "        # Add 1 to the distance if these two characters are not equal\n",
    "        if seq1[i] != seq2[i]:\n",
    "            distance += 1\n",
    "    # Return the final count of differences\n",
    "    return 1 - distance/L\n",
    "#     return distance\n",
    "\n",
    "def get_long_osk(alignements_path, threshold):\n",
    "    sequences = {}\n",
    "    for alignement in alignements_path:\n",
    "        taxid = alignement.split('/')[-1].split('.')[0]\n",
    "        handle = SeqIO.parse(alignement, 'fasta')\n",
    "        sequences[taxid] = [s for s in handle]\n",
    "    long_oskars = []\n",
    "    for taxid in sequences:\n",
    "        if len(sequences[taxid]) > 1:\n",
    "            len_first_block = 0\n",
    "            met = 0\n",
    "            for i in range(len(sequences[taxid][0])):\n",
    "                if met == len(sequences[taxid]):\n",
    "                    break\n",
    "                for j in range(len(sequences[taxid])):\n",
    "                    if sequences[taxid][j][i] == 'M':\n",
    "                        met += 1\n",
    "                len_first_block += 1\n",
    "            if len_first_block > threshold:\n",
    "                long_oskars += sequences[taxid]\n",
    "    return long_oskars\n",
    "\n",
    "def group_sequences(sources):\n",
    "    # Sources definition: [('ID', DataFrame), ('ID', Dataframe)....]\n",
    "    \n",
    "    sequences = {}\n",
    "    for source in sources:\n",
    "        for taxid, oskar_ids in source[1][source[1]['oskar_hits'] != \"None\"][['tax_id', 'oskar_hits']].values:\n",
    "            if taxid not in sequences:\n",
    "                sequences[taxid] = []\n",
    "            sequences[taxid] += [(source[0], i.strip()) for i in oskar_ids.split(',')]\n",
    "    return sequences\n",
    "\n",
    "def get_all_sequences(sequences, sources):\n",
    "    all_sequences = []\n",
    "    \n",
    "    name2seq = {}\n",
    "\n",
    "    for source in sources:\n",
    "        fasta = [s for s in SeqIO.parse(source[1], 'fasta')]\n",
    "        for seq in fasta:\n",
    "            if seq.name in name2seq:\n",
    "                print(\"ERROR SEQUENCE ALREADY EXISTS !\")\n",
    "                return False\n",
    "            name2seq[seq.name] = seq\n",
    "            \n",
    "    for taxid in sequences:\n",
    "        for origin, seq in sequences[taxid]:\n",
    "            s = name2seq[seq]\n",
    "            s.description += '|' + origin\n",
    "            all_sequences.append([origin, s])\n",
    " \n",
    "    return all_sequences\n",
    "\n",
    "def make_tmp_seq_groups(sequences, sources):\n",
    "    # sources ->   [('ID', fasta_path), ('ID', fasta_path)....]\n",
    "    name2seq = {}\n",
    "\n",
    "    for source in sources:\n",
    "        fasta = [s for s in SeqIO.parse(source[1], 'fasta')]\n",
    "        for seq in fasta:\n",
    "            if seq.name in name2seq:\n",
    "                print(\"ERROR SEQUENCE ALREADY EXISTS !\")\n",
    "                return False\n",
    "            name2seq[seq.name] = seq\n",
    "\n",
    "    tmp = './tmp/oskars'\n",
    "    if not os.path.isdir('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "    if not os.path.isdir(tmp):\n",
    "        os.mkdir(tmp)\n",
    "    sequences_groups = []\n",
    "    for taxid in sequences:\n",
    "        if len(sequences[taxid]) > 1:\n",
    "            tmpseqs = []\n",
    "            for origin, seq in sequences[taxid]:\n",
    "                s = name2seq[seq]\n",
    "                s.description += '|' + origin\n",
    "                tmpseqs.append(s)\n",
    "            outpath = os.path.join(tmp, '{}.fasta'.format(taxid))\n",
    "            sequences_groups.append(outpath)\n",
    "            f = open(outpath, 'w')\n",
    "            SeqIO.write(tmpseqs, f, 'fasta')\n",
    "    return sequences_groups\n",
    "            \n",
    "def muscle_align(inpath):\n",
    "    \"Command line wrapper for muscle, muscle needs to be in your path !\"\n",
    "    outpath = inpath.replace('fasta', 'aligned.fasta')\n",
    "    cmd = 'muscle -in {} -out {}'.format(inpath, outpath)\n",
    "    os.system(cmd)\n",
    "    return outpath\n",
    "    \n",
    "def align_sequences(sequences_groups):\n",
    "    outpaths = []\n",
    "    for path in tqdm(sequences_groups):\n",
    "        outpath = muscle_align(path)\n",
    "        outpaths.append(outpath)\n",
    "    return outpaths\n",
    "\n",
    "def trim_alignement(alignement):\n",
    "    res = []\n",
    "    for i in range(alignement.get_alignment_length()):\n",
    "        col = alignement[:, i]\n",
    "        if '-' not in col:\n",
    "            res.append([s for s in col])\n",
    "    res = np.array(res).T\n",
    "    for i in range(len(alignement)):\n",
    "        seq = ''.join(list(res[i]))\n",
    "        alignement[i].seq = seq\n",
    "    return alignement\n",
    "    \n",
    "def make_network(alignement_path):\n",
    "    alignement = AlignIO.read(alignement_path, \"fasta\")\n",
    "    trimed = trim_alignement(alignement)\n",
    "    hammings = np.zeros((len(trimed), len(trimed)))\n",
    "    nodes = []\n",
    "    for i in range(len(trimed)):\n",
    "        nodes.append(trimed[i].name)\n",
    "        for j in range(i+1, len(trimed)):\n",
    "            seq1 = trimed[i].seq\n",
    "            seq2 = trimed[j].seq\n",
    "            hammings[i][j] = hamming_distance(seq1, seq2)\n",
    "    M = np.maximum( hammings, hammings.transpose() )\n",
    "    G = nx.from_numpy_matrix(hammings)\n",
    "    for i in G.nodes():\n",
    "        G.nodes[i]['seq_id'] = nodes[i]\n",
    "    return G, nodes\n",
    "   \n",
    "def find_clusters(alignements_path, threshold=0.9):\n",
    "    clusters = {}\n",
    "    for alignement_path in alignements_path:\n",
    "        taxid = int(alignement_path.split('/')[-1].split('.')[0])\n",
    "        clusters[taxid] = []\n",
    "        G, nodes = make_network(alignement_path)\n",
    "        toremove = []\n",
    "        for n1,n2 in G.edges():\n",
    "            if G.edges[n1, n2]['weight'] < threshold:\n",
    "                toremove.append((n1, n2))\n",
    "        for n1, n2 in toremove:\n",
    "            G.remove_edge(n1, n2)\n",
    "        for cc in nx.connected_components(G):\n",
    "            tmp = []\n",
    "            for i in cc:\n",
    "                tmp.append(nodes[i])\n",
    "            clusters[taxid].append(tmp)\n",
    "    return clusters\n",
    "\n",
    "# def filter_sequences(clusters, TSA_sequences_path, TSA_hmmer_table, GCF_sequences_path, GCF_hmmer_table, uniq_sequences):\n",
    "def filter_sequences(clusters, sources, uniq_sequences):\n",
    "    # [('TSA', TSA_seq_path, TSA_hmmer_path), ('GCF', GCF_seq .......\n",
    "    result = []\n",
    "    removed = []\n",
    "    sequences = {}\n",
    "    scores = {}\n",
    "\n",
    "    # source element is \n",
    "    # ('ID', sequence_path, hmmer_hit_object)\n",
    "    for source in sources:\n",
    "        handle = SeqIO.parse(source[1], 'fasta')\n",
    "        for s in handle:\n",
    "            sequences[s.name] = (source[0], s)\n",
    "        for hit in source[2]:\n",
    "            scores[hit.id] = hit.evalue\n",
    "    \n",
    "    for s in uniq_sequences:\n",
    "        result.append(sequences[s[1]])\n",
    "            \n",
    "    for taxid in clusters:\n",
    "        for cluster in clusters[taxid]:\n",
    "            tmp_score = []\n",
    "            for seq_id in cluster:\n",
    "                tmp_score.append([scores[seq_id], sequences[seq_id]])\n",
    "            best_seq = sorted(tmp_score, key=lambda x: x[0])\n",
    "            result.append(best_seq[0][1])\n",
    "            removed += best_seq[1:]\n",
    "    return result, removed\n",
    "\n",
    "def clean_sequences(sequences):\n",
    "    cleaned = []\n",
    "    for seq in sequences:\n",
    "        tmp = \"\"\n",
    "        pos = 0\n",
    "        for l in seq.seq:\n",
    "            if l != 'X':\n",
    "                tmp += l\n",
    "            elif pos < 550:\n",
    "                tmp += l\n",
    "            else:\n",
    "                break\n",
    "            pos += 1\n",
    "        seq.seq = Seq.Seq(tmp)\n",
    "        cleaned.append(seq)\n",
    "    return cleaned\n",
    "\n",
    "def decorate_sequences(sequences, TSA_metadatas, GCF_metadatas, GCA_metadatas=None):\n",
    "    results = []\n",
    "    for s in sequences:\n",
    "        tag = s[0]\n",
    "        seq = s[1]\n",
    "        if tag == 'TSA':\n",
    "            TSA_ID = seq.name[0:6]\n",
    "            metadata = TSA_metadatas[TSA_metadatas['tsa_abrv'] == TSA_ID][['species', 'family_name', 'order_name']].values[0]\n",
    "        if tag == 'GCF':\n",
    "            if len(seq.description.split('|')) != 4:\n",
    "                GCF_specie = seq.description.split('[')[-1].split(']')[0]\n",
    "                GCF_specie = \" \".join(GCF_specie.split(' ')[:2])\n",
    "                metadata = GCF_metadatas[GCF_metadatas['species'] == GCF_specie][['species', 'family_name', 'order_name']].values[0]\n",
    "            else:\n",
    "                metadata = seq.description.split('|')\n",
    "        if tag == 'GCA':\n",
    "            if len(seq.description.split('|')) != 4:\n",
    "                try:\n",
    "                    GCA_specie = seq.description.split('[')[-1].split(']')[0]\n",
    "                    metadata = GCA_metadatas[GCA_metadatas['oskar_hits'].str.contains(GCA_specie)][['species', 'family_name', 'order_name']].values[0]\n",
    "                except:\n",
    "                    print(GCA_specie)\n",
    "            else:\n",
    "                metadata = seq.description.split('|')\n",
    "        seq.description = \"{}|{}|{}|{}\".format(metadata[0], metadata[1], metadata[2], tag)\n",
    "        results.append((metadata[1], metadata[2], seq))\n",
    "    results = sorted(results, key=lambda x: (x[1], x[0]))\n",
    "    results = [x[2] for x in results]\n",
    "    return results\n",
    "\n",
    "def remove_sequences(fasta_file, sequences, source=None):\n",
    "    if source:\n",
    "        check = [i[1] for i in sequences if i[0] == source]\n",
    "    else:\n",
    "        check = [i[1] for i in sequences]\n",
    "    \n",
    "    results = []\n",
    "    handle = SeqIO.parse(fasta_file, 'fasta')\n",
    "    for seq in handle:\n",
    "        if seq.name not in check:\n",
    "            results.append(seq)\n",
    "    f = open(fasta_file, 'w')\n",
    "    SeqIO.write(results, f, 'fasta')\n",
    "    f.close()\n",
    "    print(\"Sequences saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence list to be removed. Those sequences did not align with the rest of the oskar sequences despite matching the LOTUS + OSK exclusion criteria. They might be degenerate oskar sequences, but cannot be used for further processing as alignement is impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_removed = [\n",
    "    ('TSA', 'GCCL01027819.1_4'),\n",
    "    ('TSA', 'GGKM01011220.1_4'),\n",
    "    ('TSA', 'GFTU01001347.1_3'),\n",
    "    ('TSA', 'GAEO01004319.1_2'),\n",
    "    ('GCA', 'g35383.t1'),\n",
    "    ('GCA', 'g4008.t1'),\n",
    "    ('GCA', 'g1846.t1'),\n",
    "    ('TSA', 'GBYD01074583.1_1'),\n",
    "    ('TSA', 'GBYD01074580.1_3'),\n",
    "    ('TSA', 'GHJE01032141.1_5')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /!\\ Now processing all oskar sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first group the sequences by unique taxa\n",
    "sequences = group_sequences([\n",
    "    ('TSA', TSA_results),\n",
    "    ('GCF', GCF_results),\n",
    "    ('GCA', GCA_results),\n",
    "])\n",
    "\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create fasta files for each taxa, only if this taxa has more than one sequence\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [('TSA', TSA_sequences),\n",
    "     ('GCF', GCF_sequences),\n",
    "     ('GCA', GCA_sequences)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lblondel/anaconda3/envs/datascience/lib/python3.5/site-packages/ipykernel_launcher.py:111: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e13da9fa954e52b58e9765f73677f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We use Muscle to align those sequences (from the fasta files we just created)\n",
    "alignements_path = align_sequences(sequences_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Long Oskar alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_oskars = get_long_osk(alignements_path, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SeqIO.write(long_oskars, '../Data/01_Oskar_identification/oskar_tracker_results/long_oskar.fasta', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUSCLE v3.8.31 by Robert C. Edgar\n",
      "\n",
      "http://www.drive5.com/muscle\n",
      "This software is donated to the public domain.\n",
      "Please cite: Edgar, R.C. Nucleic Acids Res 32(5), 1792-97.\n",
      "\n",
      "long_oskar 99 seqs, max length 1641, avg  length 716\n",
      "00:00:00     24 MB(2%)  Iter   1  100.00%  K-mer dist pass 1\n",
      "00:00:00     24 MB(2%)  Iter   1  100.00%  K-mer dist pass 2\n",
      "00:00:01     84 MB(6%)  Iter   1  100.00%  Align node\n",
      "00:00:01     84 MB(6%)  Iter   1  100.00%  Root alignment\n",
      "00:00:03     84 MB(6%)  Iter   2  100.00%  Refine tree\n",
      "00:00:03     84 MB(6%)  Iter   2  100.00%  Root alignment\n",
      "00:00:03     84 MB(6%)  Iter   2  100.00%  Root alignment\n",
      "00:00:06     84 MB(6%)  Iter   3  100.00%  Refine biparts\n",
      "00:00:09     84 MB(6%)  Iter   4  100.00%  Refine biparts\n",
      "00:00:09     84 MB(6%)  Iter   5  100.00%  Refine biparts\n",
      "00:00:09     84 MB(6%)  Iter   5  100.00%  Refine biparts\n",
      "00:00:10     84 MB(6%)  Iter   6  100.00%  Refine biparts\n",
      "00:00:10     84 MB(6%)  Iter   7  100.00%  Refine biparts\n",
      "00:00:10     84 MB(6%)  Iter   8  100.00%  Refine biparts\n",
      "00:00:10     84 MB(6%)  Iter   9  100.00%  Refine biparts\n",
      "00:00:18     84 MB(6%)  Iter  10  100.00%  Refine biparts\n",
      "00:00:25     84 MB(6%)  Iter  11  100.00%  Refine biparts\n",
      "00:00:25     84 MB(6%)  Iter  12  100.00%  Refine biparts\n",
      "00:00:25     84 MB(6%)  Iter  12  100.00%  Refine biparts\n"
     ]
    }
   ],
   "source": [
    "!muscle -in ../Data/01_Oskar_identification/oskar_tracker_results/long_oskar.fasta -out ../Data/01_Oskar_identification/oskar_tracker_results/long_oskar.aligned.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Oskar alignment no with duplicates and isoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use graph theory (simple edge removal by threshold) to find sequences cluster within the sequence group\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter the sequences keeping the best E-value sequence for each cluster group,\n",
    "# and add back the sequences that were unique already\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('TSA', TSA_sequences, TSA_hmmer),\n",
    "                                                ('GCF', GCF_sequences, GCF_hmmer),\n",
    "                                                ('GCA', GCA_sequences, GCA_hmmer)\n",
    "                                                ], \n",
    "                                      uniq_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decorate the sequences for further processing with species, family and order name as well as a datatype tag \"TSA\"\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sequences = clean_sequences(decorated_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save our sequence group\n",
    "f = open(filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "remove_sequences(filtered_outpath, sequences_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sequences\n",
      "TSA: 325\n",
      "GCF: 98\n",
      "GCA: 99\n",
      "Total: 522\n",
      "The algorithm filtered out 143 sequences\n",
      "Final sequence count is 379. TSA:217 | GCF:72 | GCA:90\n"
     ]
    }
   ],
   "source": [
    "handle = SeqIO.parse(TSA_sequences, 'fasta')\n",
    "total_seq = 0\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "\n",
    "tsa_rem = total_seq\n",
    "handle = SeqIO.parse(GCF_sequences, 'fasta')\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "\n",
    "gcf_rem = total_seq - tsa_rem\n",
    "handle = SeqIO.parse(GCA_sequences, 'fasta')\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "    \n",
    "gca_rem = total_seq - (gcf_rem + tsa_rem)\n",
    "\n",
    "print(\"Starting sequences\")\n",
    "print(\"TSA:\", tsa_rem)\n",
    "print(\"GCF:\", gcf_rem)\n",
    "print(\"GCA:\", gca_rem)\n",
    "print(\"Total:\", total_seq)\n",
    "  \n",
    "handle = SeqIO.parse(filtered_outpath, 'fasta')\n",
    "filtered_seq = 0\n",
    "tsa_keep = 0\n",
    "gcf_keep = 0\n",
    "gca_keep = 0\n",
    "for s in handle:\n",
    "    filtered_seq += 1\n",
    "    tag = s.description.split('|')[-1]\n",
    "    if tag == 'TSA':\n",
    "        tsa_keep += 1\n",
    "    elif tag == 'GCF':\n",
    "        gcf_keep += 1\n",
    "    elif tag == 'GCA':\n",
    "        gca_keep += 1\n",
    "\n",
    "        \n",
    "print(\"The algorithm filtered out {} sequences\".format(total_seq-filtered_seq))\n",
    "print(\"Final sequence count is {}. TSA:{} | GCF:{} | GCA:{}\".format(filtered_seq, tsa_keep, gcf_keep, gca_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Oskar alignment no with duplicates and isoforms for TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lblondel/anaconda3/envs/datascience/lib/python3.5/site-packages/ipykernel_launcher.py:111: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de9b19b3b434896ae69f097b3e65ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Redoing the same process but with only one source type at a time for count purposes\n",
    "\n",
    "# Doing TSA\n",
    "sequences = group_sequences([\n",
    "    ('TSA', TSA_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [('TSA', TSA_sequences),\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('TSA', TSA_sequences, TSA_hmmer),\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results, GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(TSA_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "print(\"Sequences saved !\")\n",
    "remove_sequences(TSA_filtered_outpath, sequences_removed, source='TSA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Oskar alignment no with duplicates and isoforms for GCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lblondel/anaconda3/envs/datascience/lib/python3.5/site-packages/ipykernel_launcher.py:111: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62cd5b1531b4f5090d9993a158eec13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Doing GCF\n",
    "sequences = group_sequences([\n",
    "    ('GCF', GCF_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [\n",
    "     ('GCF', GCF_sequences),\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('GCF', GCF_sequences, GCF_hmmer),\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(GCF_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "print(\"Sequences saved !\")\n",
    "remove_sequences(GCF_filtered_outpath, sequences_removed, source='GCF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Oskar alignment no with duplicates and isoforms for GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lblondel/anaconda3/envs/datascience/lib/python3.5/site-packages/ipykernel_launcher.py:111: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7074d716136d4f2c9d405a262a015c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Doing GCA\n",
    "sequences = group_sequences([\n",
    "    ('GCA', GCA_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [\n",
    "     ('GCA', GCA_sequences)\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('GCA', GCA_sequences, GCA_hmmer)\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(GCA_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "remove_sequences(GCA_filtered_outpath, sequences_removed, source='GCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all Oskar sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a file of all the sequences\n",
    "sequences = group_sequences([\n",
    "    ('TSA', TSA_results),\n",
    "    ('GCF', GCF_results),\n",
    "    ('GCA', GCA_results),\n",
    "])\n",
    "\n",
    "all_sequences = get_all_sequences(\n",
    "    sequences,\n",
    "    [('TSA', TSA_sequences),\n",
    "     ('GCF', GCF_sequences),\n",
    "     ('GCA', GCA_sequences)\n",
    "    ])\n",
    "\n",
    "decorated_sequences = decorate_sequences(all_sequences, TSA_results, GCF_results ,GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(all_seq_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Align the final result with hmmalign and refine with muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hmmalign ../Data/Oskar_hmm/V3/OSKAR_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.fasta > ../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.hmmaligned.sto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = SeqIO.parse('../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.hmmaligned.sto', 'stockholm')\n",
    "seqs = [i for i in records]\n",
    "SeqIO.write(seqs, '../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.hmmaligned.fasta', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUSCLE v3.8.31 by Robert C. Edgar\n",
      "\n",
      "http://www.drive5.com/muscle\n",
      "This software is donated to the public domain.\n",
      "Please cite: Edgar, R.C. Nucleic Acids Res 32(5), 1792-97.\n",
      "\n",
      "00:07:45     67 MB(5%)  Iter   1  100.00%  Refine biparts\n",
      "00:13:16     69 MB(5%)  Iter   2  100.00%  Refine biparts\n",
      "00:17:40     73 MB(5%)  Iter   3  100.00%  Refine biparts\n",
      "00:19:33     75 MB(5%)  Iter   4  100.00%  Refine biparts\n",
      "00:19:33     75 MB(5%)  Iter   4  100.00%  Refine biparts\n"
     ]
    }
   ],
   "source": [
    "!muscle -in ../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.hmmaligned.fasta -out ../Data/01_Oskar_identification/oskar_tracker_results/oskar_all.aligned.fasta -refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hmmalign ../Data/Oskar_hmm/V3/OSKAR_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.fasta > ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.sto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = SeqIO.parse('../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.sto', 'stockholm')\n",
    "seqs = [i for i in records]\n",
    "SeqIO.write(seqs, '../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.fasta', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUSCLE v3.8.31 by Robert C. Edgar\n",
      "\n",
      "http://www.drive5.com/muscle\n",
      "This software is donated to the public domain.\n",
      "Please cite: Edgar, R.C. Nucleic Acids Res 32(5), 1792-97.\n",
      "\n",
      "00:02:18     49 MB(3%)  Iter   1  100.00%  Refine biparts\n",
      "00:03:55     51 MB(4%)  Iter   2  100.00%  Refine biparts\n",
      "00:04:52     52 MB(4%)  Iter   3  100.00%  Refine biparts\n",
      "00:04:52     52 MB(4%)  Iter   3  100.00%  Refine biparts\n"
     ]
    }
   ],
   "source": [
    "!muscle -in ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.fasta -out ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta -refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the alignement if needs be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "### Save OSK and LOTUS domains as separate alignements \n",
    "- OSK: oskar_filtered.aligned.OSK_domain.fasta\n",
    "- LOTUS: oskar_filtered.aligned.LOTUS_domain.fasta\n",
    "\n",
    "### Regenerate the OSK and LOTUS hmm and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.OSK_domain.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/OSK_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned.OSK_domain   378   663   198     4.05  0.590 \n",
      "\n",
      "# CPU time: 0.30u 0.00s 00:00:00.30 Elapsed: 00:00:00.30\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/OSK_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.OSK_domain.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.LOTUS_domain.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/LOTUS_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned.LOTUS_domain   378   167   100     7.37  0.590 \n",
      "\n",
      "# CPU time: 0.17u 0.00s 00:00:00.17 Elapsed: 00:00:00.17\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/LOTUS_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.LOTUS_domain.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/OSKAR_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned   378  2483   999    10.15  0.590 \n",
      "\n",
      "# CPU time: 1.29u 0.00s 00:00:01.29 Elapsed: 00:00:01.29\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/OSKAR_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE V to the version you are working on\n",
    "!mkdir ../Data/Oskar_hmm/V4\n",
    "\n",
    "!cp ../Data/01_Oskar_identification/oskar_tracker_results/*.hmm ../Data/Oskar_hmm/V4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current version as search files\n",
    "!cp ../Data/Oskar_hmm/V4/*.hmm ../Data/Oskar_hmm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Metadata table of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_type(x):\n",
    "    if 'GCA' in x:\n",
    "        return \"GCA\"\n",
    "    elif 'GCF' in x:\n",
    "        return 'GCF'\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def TSA_code(x):\n",
    "    return x[:4]\n",
    "\n",
    "def count_hits(x):\n",
    "    if x != \"None\":\n",
    "        return len(x.split(','))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def make_matcher(match, results):\n",
    "    mapping = {}\n",
    "    for ids, oskar in results[[match, 'oskar_hits']].values:\n",
    "        if oskar != \"None\":\n",
    "            for osk in oskar.split(','):\n",
    "                mapping[osk] = ids\n",
    "    return mapping\n",
    "\n",
    "def get_filtered_hits(fasta_path, mapping):\n",
    "    handle = SeqIO.parse(fasta_path, 'fasta')\n",
    "    sequences = [s for s in handle]\n",
    "    tmp = []\n",
    "    for s in sequences:\n",
    "        spl = s.description.split('|')\n",
    "        seq_id = spl[0].split(' ')[0]\n",
    "        specie = ' '.join(spl[0].split(' ')[1:])\n",
    "        family = spl[1]\n",
    "        order = spl[2]\n",
    "        source = spl[3]\n",
    "        source_id = mapping[seq_id]\n",
    "        tmp.append([source_id, seq_id, specie, family, order, source])\n",
    "    oskar_infos = pd.DataFrame(tmp, columns=['id', 'Sequence_id', 'Specie', 'Family', 'order_name', 'Source'])\n",
    "    return oskar_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Table S1 that provide general information on Genomes and Transcriptomes used for Oskar exaustive collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_TSA = pd.read_csv(\"../Data/01_Oskar_identification/2019/transcriptome_insect_database.csv\")\n",
    "source_genome = pd.read_csv(\"../Data/01_Oskar_identification/2019/genome_insect_database.csv\")\n",
    "source_genome['source'] = source_genome['genome_id'].map(genome_type)\n",
    "source_TSA['id'] = source_TSA['tsa_id']\n",
    "source_TSA['source'] = \"TSA\"\n",
    "# source_TSA['tsa_code'] = source_TSA['tsa_id'].map(TSA_code)\n",
    "source_TSA = source_TSA[['id', 'tax_id', 'species', 'family_name', 'order_name', 'source']]\n",
    "source_genome['id'] = source_genome['genome_id']\n",
    "source_genome = source_genome[['id', 'tax_id', 'species', 'family_name', 'order_name', 'source']]\n",
    "ncbi_metadata = source_TSA.append(source_genome).reset_index()\n",
    "ncbi_metadata.to_csv('../Data/Tables/Table_S1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Table `search_results.csv` that summarizes the oskar hits per genome and transcriptome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSA_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_results.csv')\n",
    "TSA_mapping = make_matcher('tsa_id', TSA_search_results)\n",
    "TSA_search_results['id'] = TSA_search_results['tsa_id']\n",
    "TSA_search_results['source'] = 'TSA'\n",
    "# TSA_search_results = TSA_search_results[TSA_search_results['oskar_hits'] != 'None']\n",
    "TSA_search_results['hits'] = TSA_search_results['oskar_hits'].map(count_hits)\n",
    "TSA_search_results = TSA_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "TSA_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/TSA_V3/tsa_oskar_filtered.fasta', TSA_mapping)\n",
    "TSA_hits = TSA_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "TSA_hits['filtered_hits'] = TSA_hits['Specie']\n",
    "del(TSA_hits['Specie'])\n",
    "TSA_search_results = TSA_search_results.merge(TSA_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCF_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_results.csv')\n",
    "GCF_mapping = make_matcher('genome_id', GCF_search_results)\n",
    "GCF_search_results['id'] = GCF_search_results['genome_id']\n",
    "GCF_search_results['source'] = 'GCF'\n",
    "# GCF_search_results = GCF_search_results[GCF_search_results['oskar_hits'] != 'None']\n",
    "GCF_search_results['hits'] = GCF_search_results['oskar_hits'].map(count_hits)\n",
    "GCF_search_results = GCF_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "GCF_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/GCF_V3/gcf_oskar_filtered.fasta', GCF_mapping)\n",
    "GCF_hits = GCF_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "GCF_hits['filtered_hits'] = GCF_hits['Specie']\n",
    "del(GCF_hits['Specie'])\n",
    "GCF_search_results = GCF_search_results.merge(GCF_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_results.csv')\n",
    "GCA_mapping = make_matcher('genome_id', GCA_search_results)\n",
    "GCA_search_results['id'] = GCA_search_results['genome_id']\n",
    "GCA_search_results['source'] = 'GCA'\n",
    "# GCA_search_results = GCA_search_results[GCA_search_results['oskar_hits'] != 'None']\n",
    "GCA_search_results['hits'] = GCA_search_results['oskar_hits'].map(count_hits)\n",
    "GCA_search_results = GCA_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "GCA_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/GCA_V3/gca_oskar_filtered.fasta', GCA_mapping)\n",
    "GCA_hits = GCA_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "GCA_hits['filtered_hits'] = GCA_hits['Specie']\n",
    "del(GCA_hits['Specie'])\n",
    "GCA_search_results = GCA_search_results.merge(GCA_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_metadata = TSA_search_results.append(GCF_search_results).append(GCA_search_results).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_metadata.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e019b7bf69a4c53b62891d408aa04db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_b400a06d86e34100a60705f09f496b07",
       "max": 58,
       "style": "IPY_MODEL_2c1f2a80935e48afbb06699d1e29e2d5",
       "value": 58
      }
     },
     "1975a15a5965496bb795cdc46985cdfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1b0983cbc0bc423986f763956e6cc84e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_ceca937ba067403b91fe405673692aa8",
       "max": 15,
       "style": "IPY_MODEL_35bab25c415e43dba2656b4496918136",
       "value": 15
      }
     },
     "21837378a72549e0b690d575c1fb41c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26bd8d90763d4a37bfd829d069c9550e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26e1df1a8e704267b203db29c5542393": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a96f1a2c6744bf495f1f001a5b54c94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2c1f2a80935e48afbb06699d1e29e2d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "35bab25c415e43dba2656b4496918136": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "461ab97d07764ca09478a64e84e70137": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46d7832b4cfc4822aa7de1c300d48e3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "529811a7da2c4cc3aff3aedb1ee8e098": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0e019b7bf69a4c53b62891d408aa04db",
        "IPY_MODEL_92e17df1d98a44c69f431b4632428519"
       ],
       "layout": "IPY_MODEL_deabdac315d74b50a9ba2e2451220218"
      }
     },
     "57e115dabfd24b878fc53b3c7498ae9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a1e5c6954353459ab08683be278460c5",
        "IPY_MODEL_add6fed060384a09b19faef59f8780fb"
       ],
       "layout": "IPY_MODEL_26bd8d90763d4a37bfd829d069c9550e"
      }
     },
     "677a471c357647b087c9aaade126d078": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6e04cf49e249469fb51679f5aabed0e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89f60071c2a44bc8b510fbb1851ce3fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8af75a4442ba4e16b614005f9f1d306a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c02866d45e942d880a3e6a5f8d2d754": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_21837378a72549e0b690d575c1fb41c4",
       "style": "IPY_MODEL_df02553814fc4db9bad95f04366835db",
       "value": " 15/15 [00:00&lt;00:00, 38.22it/s]"
      }
     },
     "92e17df1d98a44c69f431b4632428519": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_677a471c357647b087c9aaade126d078",
       "style": "IPY_MODEL_adcf761ee5a34b9f9f945c4afd170da8",
       "value": " 58/58 [00:05&lt;00:00, 11.43it/s]"
      }
     },
     "a1e5c6954353459ab08683be278460c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_f51dfcb73e094c2d8c5ccb840846fc3a",
       "max": 3,
       "style": "IPY_MODEL_bc6028aef49f47e9af7485f40085ab4f",
       "value": 3
      }
     },
     "adcf761ee5a34b9f9f945c4afd170da8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "add6fed060384a09b19faef59f8780fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_461ab97d07764ca09478a64e84e70137",
       "style": "IPY_MODEL_1975a15a5965496bb795cdc46985cdfa",
       "value": " 3/3 [00:00&lt;00:00, 33.79it/s]"
      }
     },
     "b400a06d86e34100a60705f09f496b07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc6028aef49f47e9af7485f40085ab4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c3ee996e463b404b8df7dfbd7dabb356": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_6e04cf49e249469fb51679f5aabed0e5",
       "max": 88,
       "style": "IPY_MODEL_46d7832b4cfc4822aa7de1c300d48e3a",
       "value": 88
      }
     },
     "ceca937ba067403b91fe405673692aa8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "deabdac315d74b50a9ba2e2451220218": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "df02553814fc4db9bad95f04366835db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e826e464c4cd4f42bba5f5416d42beea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_89f60071c2a44bc8b510fbb1851ce3fa",
       "style": "IPY_MODEL_26e1df1a8e704267b203db29c5542393",
       "value": " 88/88 [00:27&lt;00:00,  3.24it/s]"
      }
     },
     "f51dfcb73e094c2d8c5ccb840846fc3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f6ee5b873260442296359c414ccf4e96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1b0983cbc0bc423986f763956e6cc84e",
        "IPY_MODEL_8c02866d45e942d880a3e6a5f8d2d754"
       ],
       "layout": "IPY_MODEL_2a96f1a2c6744bf495f1f001a5b54c94"
      }
     },
     "fe3a6cf5ca3b4112b671730d64f9205e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c3ee996e463b404b8df7dfbd7dabb356",
        "IPY_MODEL_e826e464c4cd4f42bba5f5416d42beea"
       ],
       "layout": "IPY_MODEL_8af75a4442ba4e16b614005f9f1d306a"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
