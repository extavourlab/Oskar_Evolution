{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oskar tracker for transcriptomes\n",
    "- Author: Savandara BESSE\n",
    "- Creation: 10-05-2019\n",
    "______________________\n",
    "\n",
    "## Required inputs\n",
    "- TSA RAW folder: '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_INPUTS' \n",
    "- TSA Oskar results : './PROTEINS/TSA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os, progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.A : Create protein sequences from TSA nucleotide sequences\n",
    "    - Unzip files\n",
    "    - Rename files \n",
    "    - Translation (with clean translation end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TSA_inputs(TSA_path, protein_path):\n",
    "    protein_list = []\n",
    "    bar = progressbar.ProgressBar()\n",
    "    if not os.path.isdir(protein_path):\n",
    "        os.mkdir(protein_path)\n",
    "        \n",
    "#     os.system('gunzip {}/*'.format(TSA_path))\n",
    "#     os.system('rename \\'s/fsa_nt$/fasta/\\' {}/*.fsa_nt'.format(TSA_path))\n",
    "    \n",
    "    for TSA in bar(os.listdir(TSA_path)):\n",
    "        INPUT = os.path.join(TSA_path, TSA)\n",
    "        OUTPUT = os.path.join(protein_path, 'translated_{}'.format(TSA))\n",
    "        os.system('transeq -sequence {} -frame 6 -trim -clean -outseq {}'.format(INPUT,OUTPUT))    \n",
    "        protein_list.append(OUTPUT)\n",
    "    return protein_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1114 of 1114) |####################| Elapsed Time: 0:49:34 Time:  0:49:34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 3.6 s, total: 6.67 s\n",
      "Wall time: 57min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "TSA_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_INPUTS'\n",
    "protein_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_PROTEIN'\n",
    "translated_TSA = build_TSA_inputs(TSA_path, protein_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (168 of 168) |######################| Elapsed Time: 0:18:03 Time:  0:18:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 830 ms, total: 1.17 s\n",
      "Wall time: 18min 3s\n"
     ]
    }
   ],
   "source": [
    "## Crustacean analysis 2017\n",
    "%%time \n",
    "TSA_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_INPUTS'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_PROTEIN'\n",
    "translated_TSA = build_TSA_inputs(TSA_path, protein_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.B: Collect GCF protein\n",
    "    - Copy GCF protein fasta files\n",
    "    - Gunzip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_Proteins(currentFolder):\n",
    "    for gFile in os.listdir(currentFolder):\n",
    "        if '_protein.faa.gz' in gFile :\n",
    "            return gFile\n",
    "        \n",
    "def build_GCF_inputs(genome_path, res_path):\n",
    "    for folder in os.listdir(genome_path): \n",
    "        if 'GCF' in folder :\n",
    "            currentFolder = os.path.join(genome_path, folder)\n",
    "            proteome = reach_Proteins(currentFolder)\n",
    "            target = os.path.join(genome_path, currentFolder, proteome)\n",
    "            os.system('cp {} {}'.format(target, res_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "genome_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/Genomes'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_PROTEIN'\n",
    "build_GCF_inputs(genome_path, protein_path)\n",
    "os.system('gunzip {}/*'.format(protein_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.C: Collect GCA protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_Genes(currentFolder):\n",
    "    all_genomic_files = [ gFile for gFile in os.listdir(currentFolder) if '_genomic.fna.gz' in gFile ]\n",
    "    if len(all_genomic_files) > 1 :\n",
    "        for gFile in all_genomic_files :\n",
    "            if ('cds' in gFile) or ('rna' in gFile):\n",
    "                pass\n",
    "            else :\n",
    "                return gFile\n",
    "    else :\n",
    "        return all_genomic_files[0]\n",
    "        \n",
    "def build_GCA_inputs(genome_path, res_path):\n",
    "    for folder in os.listdir(genome_path): \n",
    "        if 'GCA' in folder :\n",
    "            currentFolder = os.path.join(genome_path, folder)\n",
    "            proteome = reach_Genes(currentFolder)\n",
    "            target = os.path.join(genome_path, currentFolder, proteome)\n",
    "            os.system('cp {} {}'.format(target, res_path))\n",
    "\n",
    "def set_augustus_model(order, table):\n",
    "    if order == 'Diplura' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Archaeognatha':\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Odonata' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Ephemeroptera' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Plecoptera' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Orthoptera' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Phasmatodea' :\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Blattodea':\n",
    "        hmm_order = 'zootermopsis_nevadensis'\n",
    "    elif order == 'Thysanoptera' :\n",
    "        hmm_order = 'frankliniella_occidentalis'\n",
    "    elif order == 'Hemiptera' :\n",
    "        hmm_order = 'bemisia_tabaci'\n",
    "    elif order == 'Phthiraptera' :\n",
    "        hmm_order = 'pediculus_humanus'\n",
    "    elif order == 'Hymenoptera' :\n",
    "        hmm_order = 'apis_mellifera'\n",
    "    elif order == 'Strepsiptera' :\n",
    "        hmm_order = 'tribolium_castaneum'\n",
    "    elif order == 'Coleoptera' :\n",
    "        hmm_order = 'tribolium_castaneum'\n",
    "    elif order == 'Trichoptera' :\n",
    "        hmm_order = 'papilio_xuthus'\n",
    "    elif order == 'Lepidoptera' :\n",
    "        hmm_order = 'papilio_xuthus'\n",
    "    elif order == 'Siphonaptera' :\n",
    "        hmm_order = 'ctenophalides_felis'\n",
    "    elif order == 'Diptera' :\n",
    "        family = table[table['order_name'] == order ]['family_name']\n",
    "        if 'Culicidae' in family :\n",
    "            hmm_order = 'aedes'\n",
    "        elif 'Pteromalidae' in family :\n",
    "            hmm_order = 'nasonia'\n",
    "        else :\n",
    "            hmm_order = 'fly'\n",
    "    return hmm_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "genome_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/Genomes'\n",
    "res_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_INPUTS_2'\n",
    "build_GCA_inputs(genome_path, res_path)\n",
    "# os.system('gunzip {}/*'.format(res_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCA = GCA[GCA['genome_id'].str.contains('GCA')]\n",
    "GCA['pgc_mode'] = GCA['order_name'].apply(set_germ_cell_formation)\n",
    "GCA['augustus_model'] = GCA['order_name'].apply(set_augustus_model, args=(GCA,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genome_id</th>\n",
       "      <th>tax_id</th>\n",
       "      <th>species</th>\n",
       "      <th>family_id</th>\n",
       "      <th>family_name</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_name</th>\n",
       "      <th>bioproject</th>\n",
       "      <th>biosample</th>\n",
       "      <th>download_status</th>\n",
       "      <th>pgc_mode</th>\n",
       "      <th>augustus_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [genome_id, tax_id, species, family_id, family_name, order_id, order_name, bioproject, biosample, download_status, pgc_mode, augustus_model]\n",
       "Index: []"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Track Oskar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 ms, sys: 23.9 ms, total: 60.5 ms\n",
      "Wall time: 27min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lotus_model = '../Data/Oskar_hmm/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/OSK_CONSENSUS.hmm'\n",
    "protein_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_PROTEIN'\n",
    "result_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_RESULT'\n",
    "os.system('python3 execute_hmmsearch.py -a TSA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 10.9 ms, total: 23 ms\n",
      "Wall time: 6min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## Crustacean analysis \n",
    "lotus_model = '../Data/Oskar_hmm/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/OSK_CONSENSUS.hmm'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_PROTEIN'\n",
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/TSA_RESULT'\n",
    "os.system('python3 execute_hmmsearch.py -a TSA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.87 ms, sys: 3.64 ms, total: 6.51 ms\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lotus_model = '../Data/Oskar_hmm/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/OSK_CONSENSUS.hmm'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_PROTEIN'\n",
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_RESULT'\n",
    "os.system('python3 execute_hmmsearch.py -a GCF -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.36 ms, sys: 2.94 ms, total: 7.3 ms\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lotus_model = '../Data/Oskar_hmm/LOTUS_CONSENSUS.hmm'\n",
    "osk_model = '../Data/Oskar_hmm/OSK_CONSENSUS.hmm'\n",
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_PROTEIN'\n",
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_RESULT'\n",
    "os.system('python3 execute_hmmsearch.py -a GCA -p {} -l {} -o {} -r {}'.format(protein_path, lotus_model, osk_model, result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCA with OSKAR HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oskar_model = '../Data/Oskar_hmm/OSKAR_CONSENSUS.hmm'\n",
    "# hmmsearch --cpu 8 --tblout GCA_OSKAR/proteome_oskar_search.txt model proteome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Collect Oskar candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SearchIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def collect_Table(TSA, HMM):\n",
    "    for DOM in HMM :\n",
    "        if TSA in DOM :\n",
    "            return DOM\n",
    "        \n",
    "def collect_Hits(result_path, hmmerTable):\n",
    "    f = open(os.path.join(result_path, hmmerTable))\n",
    "    tmp = [ line for line in f.readlines() if '#' not in line ]\n",
    "    f.close()\n",
    "    if len(tmp) != 0 : \n",
    "        return [ hit.id for hit in SearchIO.read(hmmerTable, 'hmmer3-tab') if hit.evalue <= 0.05 ]\n",
    "    return []\n",
    "    \n",
    "\n",
    "def oskar_analysis(ID):\n",
    "    lotus_res = os.path.join(result_path, collect_Table(ID, LOTUS))\n",
    "    osk_res = os.path.join(result_path, collect_Table(ID, OSK))\n",
    "\n",
    "    lotus_hits = collect_Hits(result_path, lotus_res)\n",
    "    osk_hits = collect_Hits(result_path, osk_res)\n",
    "    oskar_hits = list(set(lotus_hits) & set(osk_hits))\n",
    "    \n",
    "    if len(oskar_hits) == 0 :\n",
    "        oskar_hits = 'None'\n",
    "    else : \n",
    "        oskar_hits = ','.join(list(set(lotus_hits) & set(osk_hits)))\n",
    "    if len(lotus_hits) == 0 :\n",
    "        lotus_hits = 'None'\n",
    "    else :\n",
    "        lotus_hits = ','.join(list(lotus_hits))\n",
    "    if len(osk_hits) == 0:\n",
    "        osk_hits = 'None'\n",
    "    else : \n",
    "        osk_hits = ','.join(list(osk_hits))\n",
    "\n",
    "    return [ID, lotus_hits, osk_hits, oskar_hits ]\n",
    "        \n",
    "PGC_mode = {\n",
    "    'Diplura':'Induction',\n",
    "    'Archaeognatha':'Induction',\n",
    "    'Zygentoma':'Induction',\n",
    "    'Odonata':'Induction',\n",
    "    'Ephemeroptera':'Induction',\n",
    "    'Zoraptera':'Induction',\n",
    "    'Dermaptera':'Induction',\n",
    "    'Plecoptera':'Induction',\n",
    "    'Orthoptera':'Induction',\n",
    "    'Mantophasmatodea':'Induction',\n",
    "    'Grylloblattodea':'Induction',\n",
    "    'Embioptera':'Induction',\n",
    "    'Phasmatodea':'Induction',\n",
    "    'Mantodea':'Induction',\n",
    "    'Blattodea':'Induction',\n",
    "    'Isoptera':'Induction',\n",
    "    'Thysanoptera':'Induction',\n",
    "    'Hemiptera':'Induction',\n",
    "    'Phthiraptera':'Induction',\n",
    "    'Psocoptera':'Induction',\n",
    "    'Hymenoptera':'Inheritance',\n",
    "    'Raphidioptera':'Inheritance',\n",
    "    'Megaloptera':'Inheritance',\n",
    "    'Neuroptera':'Inheritance',\n",
    "    'Strepsiptera':'Inheritance',\n",
    "    'Coleoptera':'Inheritance',\n",
    "    'Trichoptera':'Inheritance',\n",
    "    'Lepidoptera':'Inheritance',\n",
    "    'Siphonaptera':'Inheritance',\n",
    "    'Mecoptera':'Inheritance',\n",
    "    'Diptera':'Inheritance'\n",
    "}\n",
    "\n",
    "def set_germ_cell_formation(x):\n",
    "    return PGC_mode[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 28 ms, total: 1.53 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_RESULT'\n",
    "IDs = list(set([ TSA.split('_')[0] for TSA in os.listdir('../Data/01_Oskar_identification/hmmsearch_raw_results/TSA') ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(result_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(result_path)) if 'osk' in domain ]\n",
    "TSA_OSKAR = list(map(oskar_analysis, IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TMP = pd.DataFrame(TSA_OSKAR, columns=['tsa_abrv', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "TSA = pd.read_csv('../Data/01_Oskar_identification/2019/transcriptome_insect_database.csv')\n",
    "\n",
    "TSA['pgc_mode'] = TSA['order_name'].apply(set_germ_cell_formation)\n",
    "TSA['tsa_abrv'] = [ '{}{}'.format(TSA['tsa_id'][i][:5], TSA['tsa_id'][i].split('.')[1]) for i in range(len(TSA)) ]\n",
    "\n",
    "TSA = TSA.merge(TMP, on='tsa_abrv')\n",
    "TSA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 327 ms, sys: 37.6 ms, total: 364 ms\n",
      "Wall time: 433 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Curstacean analysis \n",
    "tsa_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/TSA_crustacea'\n",
    "IDs = list(set([ TSA.split('_')[0] for TSA in os.listdir('../Data/01_Oskar_identification/hmmsearch_raw_results/TSA_crustacea') ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(tsa_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(tsa_path)) if 'osk' in domain ]\n",
    "TSA_OSKAR = list(map(oskar_analysis, IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTSA(x):\n",
    "    x = x.replace('.1', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP = pd.DataFrame(TSA_OSKAR, columns=['TSA', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "TMP['TSA'] = TMP['TSA'].apply(formatTSA)\n",
    "TSA = pd.read_csv('../Data/01_Oskar_identification/2017/transcriptome_crustacean_database.csv')\n",
    "# TSA['pgc_mode'] = TSA['order_name'].apply(set_germ_cell_formation)\n",
    "TSA = TSA.merge(TMP, on='TSA')\n",
    "TSA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA_crustacea/tsa_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_RESULT'\n",
    "IDs = list(set([ '{}_{}'.format(GCF.split('_')[0], GCF.split('_')[1]) for GCF in os.listdir('../Data/01_Oskar_identification/hmmsearch_raw_results/GCF') ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(result_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(result_path)) if 'osk' in domain ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCF_OSKAR = list(map(oskar_analysis, IDs))\n",
    "TMP = pd.DataFrame(GCF_OSKAR, columns=['genome_id', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "\n",
    "GCF = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCF['pgc_mode'] = GCF['order_name'].apply(set_germ_cell_formation)\n",
    "\n",
    "GCF = GCF.merge(TMP, on='genome_id')\n",
    "GCF.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gca_path = '../Data/01_Oskar_identification/hmmsearch_raw_results/GCA'\n",
    "IDs = list(set([ '{}_{}'.format(GCA.split('_')[0], GCA.split('_')[1]) for GCA in os.listdir(gca_path) ]))\n",
    "LOTUS = [ domain for domain in sorted(os.listdir(gca_path)) if 'lotus' in domain ]\n",
    "OSK = [ domain for domain in sorted(os.listdir(gca_path)) if 'osk' in domain ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA_OSKAR = list(map(oskar_analysis, IDs))\n",
    "TMP = pd.DataFrame(GCA_OSKAR, columns=['genome_id', 'lotus_hits', 'osk_hits', 'oskar_hits'])\n",
    "\n",
    "GCA = pd.read_csv('../Data/01_Oskar_identification/2019/genome_insect_database.csv')\n",
    "GCA['pgc_mode'] = GCA['order_name'].apply(set_germ_cell_formation)\n",
    "GCA['augustus_model'] = GCA['order_name'].apply(set_augustus_model, args=(GCA,))\n",
    "GCA = GCA.merge(TMP, on='genome_id')\n",
    "GCA.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_results.csv', index=False, na_rep='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_genome(x, table):\n",
    "    return table[table['oskar_hits'].str.contains(x)]['genome_id'].values[0]\n",
    "\n",
    "def retrieveSeq(seqID, dataType, fastaFile):\n",
    "    for seqRecord in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        if seqID in seqRecord.id :\n",
    "            if 'TSA' in dataType :\n",
    "                seqRecord.seq = format_Seq(seqRecord.seq)\n",
    "            return seqRecord\n",
    "        \n",
    "def first_AA(seq):\n",
    "    for i in range(len(seq)):\n",
    "        if 'M' in seq[i]:\n",
    "            return i\n",
    "\n",
    "def format_Seq(seq):\n",
    "    return seq[first_AA(seq):]\n",
    "\n",
    "def save_oskar(TABLE, protein_path, dataType, result_file):\n",
    "    OSKAR_SEQ = [] \n",
    "    OSKAR_HITS = [ ID for LIST in TABLE[TABLE['oskar_hits'] != 'None']['oskar_hits'] for ID in LIST.split(',') ]\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for SEQ in bar(OSKAR_HITS): \n",
    "        if 'TSA' in dataType :\n",
    "            ID = SEQ[:4]\n",
    "        else :\n",
    "            ID = collect_genome(SEQ, TABLE)\n",
    "        for PROTEOME in os.listdir(protein_path):\n",
    "            if ID in PROTEOME :\n",
    "                FASTA = os.path.join(protein_path, PROTEOME)\n",
    "                OSKAR_SEQ.append(retrieveSeq(SEQ, dataType, FASTA))\n",
    "    SeqIO.write(OSKAR_SEQ, result_file, 'fasta')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (326 of 326) |######################| Elapsed Time: 0:08:21 Time:  0:08:21\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/savvy/DATA2/savvy/EXTAVOUR/SOURCES/TSA/TSA_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_candidates.fasta'\n",
    "save_oskar(TSA, protein_path, 'TSA', result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (98 of 98) |########################| Elapsed Time: 0:00:08 Time:  0:00:08\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCF_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_candidates.fasta'\n",
    "save_oskar(GCF, protein_path, 'GCF', result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (99 of 99) |########################| Elapsed Time: 0:00:16 Time:  0:00:16\n"
     ]
    }
   ],
   "source": [
    "protein_path = '/media/lblondel/5D1FA0DA2BE76E76/savy/SOURCES/GCA_PROTEIN'\n",
    "result_file = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_candidates.fasta'\n",
    "save_oskar(GCA, protein_path, 'GCA', result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validate candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tsa_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/final_tsa_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, tsa_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcf_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/final_gcf_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, gcf_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gca_candidates = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_candidates.fasta'\n",
    "oskar_model = '../Data/Oskar_hmm/OSKAR_CONSENSUS.hmm'\n",
    "oskar_validation_results = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/final_gca_oskar_search.txt'\n",
    "os.system('hmmsearch --cpu 8 --tblout {} {} {}'.format(oskar_validation_results, oskar_model, gca_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Oskar duplicate and isoform detection and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from Bio import SeqIO\n",
    "from Bio import SearchIO\n",
    "from Bio import Align\n",
    "from Bio import AlignIO\n",
    "from Bio import Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSA_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_results.csv')\n",
    "TSA_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_candidates.fasta'\n",
    "hmmerTable = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/final_tsa_oskar_search.txt'\n",
    "TSA_hmmer = SearchIO.read(hmmerTable, 'hmmer3-tab')\n",
    "TSA_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCF_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_results.csv')\n",
    "GCF_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_candidates.fasta'\n",
    "GCF_hmmer = SearchIO.read('../Data/01_Oskar_identification/oskar_tracker_results/GCF/final_gcf_oskar_search.txt', 'hmmer3-tab')\n",
    "GCF_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_results.csv')\n",
    "GCA_sequences = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_candidates.fasta'\n",
    "GCA_hmmer = SearchIO.read('../Data/01_Oskar_identification/oskar_tracker_results/GCA/final_gca_oskar_search.txt', 'hmmer3-tab')\n",
    "GCA_filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_outpath = '../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Hamming distance between string1 and string2.\n",
    "# string1 and string2 should be the same length.\n",
    "def hamming_distance(seq1, seq2): \n",
    "    assert(len(seq1) == len(seq2)) \n",
    "    # Start with a distance of zero, and count up\n",
    "    distance = 0.0\n",
    "    # Loop over the indices of the string\n",
    "    L = len(seq1)\n",
    "    for i in range(L):\n",
    "        # Add 1 to the distance if these two characters are not equal\n",
    "        if seq1[i] != seq2[i]:\n",
    "            distance += 1\n",
    "    # Return the final count of differences\n",
    "    return 1 - distance/L\n",
    "#     return distance\n",
    "\n",
    "def group_sequences(sources):\n",
    "    # Sources definition: [('ID', DataFrame), ('ID', Dataframe)....]\n",
    "    \n",
    "    sequences = {}\n",
    "    for source in sources:\n",
    "        for taxid, oskar_ids in source[1][source[1]['oskar_hits'] != \"None\"][['tax_id', 'oskar_hits']].values:\n",
    "            if taxid not in sequences:\n",
    "                sequences[taxid] = []\n",
    "            sequences[taxid] += [(source[0], i.strip()) for i in oskar_ids.split(',')]\n",
    "    return sequences\n",
    "\n",
    "def make_tmp_seq_groups(sequences, sources):\n",
    "    # sources ->   [('ID', fasta_path), ('ID', fasta_path)....]\n",
    "    name2seq = {}\n",
    "\n",
    "    for source in sources:\n",
    "        fasta = [s for s in SeqIO.parse(source[1], 'fasta')]\n",
    "        for seq in fasta:\n",
    "            if seq.name in name2seq:\n",
    "                print(\"ERROR SEQUENCE ALREADY EXISTS !\")\n",
    "                return False\n",
    "            name2seq[seq.name] = seq\n",
    "\n",
    "    tmp = './tmp/oskars'\n",
    "    if not os.path.isdir('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "    if not os.path.isdir(tmp):\n",
    "        os.mkdir(tmp)\n",
    "    sequences_groups = []\n",
    "    for taxid in sequences:\n",
    "        if len(sequences[taxid]) > 1:\n",
    "            tmpseqs = []\n",
    "            for origin, seq in sequences[taxid]:\n",
    "                s = name2seq[seq]\n",
    "                s.description += '|' + origin\n",
    "                tmpseqs.append(s)\n",
    "            outpath = os.path.join(tmp, '{}.fasta'.format(taxid))\n",
    "            sequences_groups.append(outpath)\n",
    "            f = open(outpath, 'w')\n",
    "            SeqIO.write(tmpseqs, f, 'fasta')\n",
    "    return sequences_groups\n",
    "            \n",
    "def muscle_align(inpath):\n",
    "    \"Command line wrapper for muscle, muscle needs to be in your path !\"\n",
    "    outpath = inpath.replace('fasta', 'aligned.fasta')\n",
    "    cmd = 'muscle -in {} -out {}'.format(inpath, outpath)\n",
    "    os.system(cmd)\n",
    "    return outpath\n",
    "    \n",
    "def align_sequences(sequences_groups):\n",
    "    outpaths = []\n",
    "    for path in tqdm(sequences_groups):\n",
    "        outpath = muscle_align(path)\n",
    "        outpaths.append(outpath)\n",
    "    return outpaths\n",
    "\n",
    "def trim_alignement(alignement):\n",
    "    res = []\n",
    "    for i in range(alignement.get_alignment_length()):\n",
    "        col = alignement[:, i]\n",
    "        if '-' not in col:\n",
    "            res.append([s for s in col])\n",
    "    res = np.array(res).T\n",
    "    for i in range(len(alignement)):\n",
    "        seq = ''.join(list(res[i]))\n",
    "        alignement[i].seq = seq\n",
    "    return alignement\n",
    "    \n",
    "def make_network(alignement_path):\n",
    "    alignement = AlignIO.read(alignement_path, \"fasta\")\n",
    "    trimed = trim_alignement(alignement)\n",
    "    hammings = np.zeros((len(trimed), len(trimed)))\n",
    "    nodes = []\n",
    "    for i in range(len(trimed)):\n",
    "        nodes.append(trimed[i].name)\n",
    "        for j in range(i+1, len(trimed)):\n",
    "            seq1 = trimed[i].seq\n",
    "            seq2 = trimed[j].seq\n",
    "            hammings[i][j] = hamming_distance(seq1, seq2)\n",
    "    M = np.maximum( hammings, hammings.transpose() )\n",
    "    G = nx.from_numpy_matrix(hammings)\n",
    "    for i in G.nodes():\n",
    "        G.nodes[i]['seq_id'] = nodes[i]\n",
    "    return G, nodes\n",
    "   \n",
    "def find_clusters(alignements_path, threshold=0.9):\n",
    "    clusters = {}\n",
    "    for alignement_path in alignements_path:\n",
    "        taxid = int(alignement_path.split('/')[-1].split('.')[0])\n",
    "        clusters[taxid] = []\n",
    "        G, nodes = make_network(alignement_path)\n",
    "        toremove = []\n",
    "        for n1,n2 in G.edges():\n",
    "            if G.edges[n1, n2]['weight'] < threshold:\n",
    "                toremove.append((n1, n2))\n",
    "        for n1, n2 in toremove:\n",
    "            G.remove_edge(n1, n2)\n",
    "        for cc in nx.connected_components(G):\n",
    "            tmp = []\n",
    "            for i in cc:\n",
    "                tmp.append(nodes[i])\n",
    "            clusters[taxid].append(tmp)\n",
    "    return clusters\n",
    "\n",
    "# def filter_sequences(clusters, TSA_sequences_path, TSA_hmmer_table, GCF_sequences_path, GCF_hmmer_table, uniq_sequences):\n",
    "def filter_sequences(clusters, sources, uniq_sequences):\n",
    "    # [('TSA', TSA_seq_path, TSA_hmmer_path), ('GCF', GCF_seq .......\n",
    "    result = []\n",
    "    removed = []\n",
    "    sequences = {}\n",
    "    scores = {}\n",
    "\n",
    "    # source element is \n",
    "    # ('ID', sequence_path, hmmer_hit_object)\n",
    "    for source in sources:\n",
    "        handle = SeqIO.parse(source[1], 'fasta')\n",
    "        for s in handle:\n",
    "            sequences[s.name] = (source[0], s)\n",
    "        for hit in source[2]:\n",
    "            scores[hit.id] = hit.evalue\n",
    "    \n",
    "    for s in uniq_sequences:\n",
    "        result.append(sequences[s[1]])\n",
    "            \n",
    "    for taxid in clusters:\n",
    "        for cluster in clusters[taxid]:\n",
    "            tmp_score = []\n",
    "            for seq_id in cluster:\n",
    "                tmp_score.append([scores[seq_id], sequences[seq_id]])\n",
    "            best_seq = sorted(tmp_score, key=lambda x: x[0])\n",
    "            result.append(best_seq[0][1])\n",
    "            removed += best_seq[1:]\n",
    "    return result, removed\n",
    "\n",
    "def clean_sequences(sequences):\n",
    "    cleaned = []\n",
    "    for seq in sequences:\n",
    "        tmp = \"\"\n",
    "        pos = 0\n",
    "        for l in seq.seq:\n",
    "            if l != 'X':\n",
    "                tmp += l\n",
    "            elif pos < 550:\n",
    "                tmp += l\n",
    "            else:\n",
    "                break\n",
    "            pos += 1\n",
    "        seq.seq = Seq.Seq(tmp)\n",
    "        cleaned.append(seq)\n",
    "    return cleaned\n",
    "\n",
    "def decorate_sequences(sequences, TSA_metadatas, GCF_metadatas, GCA_metadatas=None):\n",
    "    results = []\n",
    "    for s in sequences:\n",
    "        tag = s[0]\n",
    "        seq = s[1]\n",
    "        if tag == 'TSA':\n",
    "            TSA_ID = seq.name[0:6]\n",
    "            metadata = TSA_metadatas[TSA_metadatas['tsa_abrv'] == TSA_ID][['species', 'family_name', 'order_name']].values[0]\n",
    "        if tag == 'GCF':\n",
    "            if len(seq.description.split('|')) != 4:\n",
    "                GCF_specie = seq.description.split('[')[-1].split(']')[0]\n",
    "                metadata = GCF_metadatas[GCF_metadatas['species'] == GCF_specie][['species', 'family_name', 'order_name']].values[0]\n",
    "            else:\n",
    "                metadata = seq.description.split('|')\n",
    "        if tag == 'GCA':\n",
    "            if len(seq.description.split('|')) != 4:\n",
    "                try:\n",
    "                    GCA_specie = seq.description.split('[')[-1].split(']')[0]\n",
    "                    metadata = GCA_metadatas[GCA_metadatas['oskar_hits'].str.contains(GCA_specie)][['species', 'family_name', 'order_name']].values[0]\n",
    "                except:\n",
    "                    print(GCA_specie)\n",
    "            else:\n",
    "                metadata = seq.description.split('|')\n",
    "        seq.description = \"{}|{}|{}|{}\".format(metadata[0], metadata[1], metadata[2], tag)\n",
    "        results.append((metadata[1], metadata[2], seq))\n",
    "    results = sorted(results, key=lambda x: (x[1], x[0]))\n",
    "    results = [x[2] for x in results]\n",
    "    return results\n",
    "\n",
    "def remove_sequences(fasta_file, sequences, source=None):\n",
    "    if source:\n",
    "        check = [i[1] for i in sequences if i[0] == source]\n",
    "    else:\n",
    "        check = [i[1] for i in sequences]\n",
    "    \n",
    "    results = []\n",
    "    handle = SeqIO.parse(fasta_file, 'fasta')\n",
    "    for seq in handle:\n",
    "        if seq.name not in check:\n",
    "            results.append(seq)\n",
    "    f = open(fasta_file, 'w')\n",
    "    SeqIO.write(results, f, 'fasta')\n",
    "    f.close()\n",
    "    print(\"Sequences saved !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'efg', 4), ('a', 'gfg', 3), ('b', 'eff', 5), ('c', 'dcf', 6)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\n",
    "    ('a','gfg',3),\n",
    "    ('a','efg',4),\n",
    "    ('b','eff',5),\n",
    "    ('c','dcf',6)\n",
    "]\n",
    "\n",
    "sorted(X, key=lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence list to be removed. Those sequences did not align with the rest of the oskar sequences despite matching the LOTUS + OSK exclusion criteria. They might be degenerate oskar sequences, but cannot be used for further processing as alignement is impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_removed = [\n",
    "    ('TSA', 'GCCL01027819.1_4'),\n",
    "    ('TSA', 'GGKM01011220.1_4'),\n",
    "    ('TSA', 'GFTU01001347.1_3'),\n",
    "    ('TSA', 'GAEO01004319.1_2'),\n",
    "    ('GCA', 'g35383.t1'),\n",
    "    ('TSA', 'GBYD01074583.1_1'),\n",
    "    ('TSA', 'GBYD01074580.1_3'),\n",
    "    ('TSA', 'GHJE01032141.1_5')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now processing all oskar sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first group the sequences by unique taxa\n",
    "sequences = group_sequences([\n",
    "    ('TSA', TSA_results),\n",
    "    ('GCF', GCF_results),\n",
    "    ('GCA', GCA_results),\n",
    "])\n",
    "\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create fasta files for each taxa, only if this taxa has more than one sequence\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [('TSA', TSA_sequences),\n",
    "     ('GCF', GCF_sequences),\n",
    "     ('GCA', GCA_sequences)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xqua/opt/anaconda3/envs/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:68: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c59ee66215b4aa982986345ea41ebdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We use Muscle to align those sequences (from the fasta files we just created)\n",
    "alignements_path = align_sequences(sequences_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use graph theory (simple edge removal by threshold) to find sequences cluster within the sequence group\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter the sequences keeping the best E-value sequence for each cluster group,\n",
    "# and add back the sequences that were unique already\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('TSA', TSA_sequences, TSA_hmmer),\n",
    "                                                ('GCF', GCF_sequences, GCF_hmmer),\n",
    "                                                ('GCA', GCA_sequences, GCA_hmmer)\n",
    "                                                ], \n",
    "                                      uniq_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decorate the sequences for further processing with species, family and order name as well as a datatype tag \"TSA\"\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sequences = clean_sequences(decorated_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save our sequence group\n",
    "f = open(filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "remove_sequences(filtered_outpath, sequences_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sequences\n",
      "TSA: 326\n",
      "GCF: 98\n",
      "GCA: 99\n",
      "Total: 523\n",
      "The algorithm filtered out 143 sequences\n",
      "Final sequence count is 380. TSA:213 | GCF:75 | GCA:92\n"
     ]
    }
   ],
   "source": [
    "handle = SeqIO.parse(TSA_sequences, 'fasta')\n",
    "total_seq = 0\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "\n",
    "tsa_rem = total_seq\n",
    "handle = SeqIO.parse(GCF_sequences, 'fasta')\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "\n",
    "gcf_rem = total_seq - tsa_rem\n",
    "handle = SeqIO.parse(GCA_sequences, 'fasta')\n",
    "for s in handle:\n",
    "    total_seq += 1\n",
    "    \n",
    "gca_rem = total_seq - (gcf_rem + tsa_rem)\n",
    "\n",
    "print(\"Starting sequences\")\n",
    "print(\"TSA:\", tsa_rem)\n",
    "print(\"GCF:\", gcf_rem)\n",
    "print(\"GCA:\", gca_rem)\n",
    "print(\"Total:\", total_seq)\n",
    "  \n",
    "handle = SeqIO.parse(filtered_outpath, 'fasta')\n",
    "filtered_seq = 0\n",
    "tsa_keep = 0\n",
    "gcf_keep = 0\n",
    "gca_keep = 0\n",
    "for s in handle:\n",
    "    filtered_seq += 1\n",
    "    tag = s.description.split('|')[-1]\n",
    "    if tag == 'TSA':\n",
    "        tsa_keep += 1\n",
    "    elif tag == 'GCF':\n",
    "        gcf_keep += 1\n",
    "    elif tag == 'GCA':\n",
    "        gca_keep += 1\n",
    "\n",
    "        \n",
    "print(\"The algorithm filtered out {} sequences\".format(total_seq-filtered_seq))\n",
    "print(\"Final sequence count is {}. TSA:{} | GCF:{} | GCA:{}\".format(filtered_seq, tsa_keep, gcf_keep, gca_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xqua/opt/anaconda3/envs/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:68: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2b1ae49e874f01944fa4ee57fed6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Redoing the same process but with only one source type at a time for count purposes\n",
    "\n",
    "# Doing TSA\n",
    "sequences = group_sequences([\n",
    "    ('TSA', TSA_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [('TSA', TSA_sequences),\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('TSA', TSA_sequences, TSA_hmmer),\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results, GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(TSA_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "print(\"Sequences saved !\")\n",
    "remove_sequences(TSA_filtered_outpath, sequences_removed, source='TSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xqua/opt/anaconda3/envs/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:68: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd68337e2694c24906c2edb0373b29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Doing GCF\n",
    "sequences = group_sequences([\n",
    "    ('GCF', GCF_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [\n",
    "     ('GCF', GCF_sequences),\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('GCF', GCF_sequences, GCF_hmmer),\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(GCF_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "print(\"Sequences saved !\")\n",
    "remove_sequences(GCF_filtered_outpath, sequences_removed, source='GCF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xqua/opt/anaconda3/envs/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:68: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9cec8f215c4632a0179c3f0f31ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences saved !\n"
     ]
    }
   ],
   "source": [
    "# Doing GCA\n",
    "sequences = group_sequences([\n",
    "    ('GCA', GCA_results),\n",
    "])\n",
    "uniq_sequences = [sequences[k][0] for k in sequences if len(sequences[k]) == 1]\n",
    "sequences_groups = make_tmp_seq_groups(\n",
    "    sequences,\n",
    "    [\n",
    "     ('GCA', GCA_sequences)\n",
    "    ])\n",
    "alignements_path = align_sequences(sequences_groups)\n",
    "clusters = find_clusters(alignements_path, threshold=0.80)\n",
    "filtered_sequences, removed_sequences = filter_sequences(clusters, [\n",
    "                                                ('GCA', GCA_sequences, GCA_hmmer)\n",
    "                                                ], \n",
    "                                      uniq_sequences)\n",
    "decorated_sequences = decorate_sequences(filtered_sequences, TSA_results, GCF_results ,GCA_results)\n",
    "cleaned_sequences = clean_sequences(decorated_sequences)\n",
    "\n",
    "# We save our sequence group\n",
    "f = open(GCA_filtered_outpath, 'w')\n",
    "SeqIO.write(cleaned_sequences, f, 'fasta')\n",
    "f.close()\n",
    "remove_sequences(GCA_filtered_outpath, sequences_removed, source='GCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align the final result with hmmalign and refine with muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hmmalign ../Data/Oskar_hmm/OSKAR_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.fasta > ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.sto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "i = open('../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.sto')\n",
    "o = open('../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.fasta', 'w')\n",
    "\n",
    "SeqIO.convert(i, 'stockholm', o, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUSCLE v3.8.1551 by Robert C. Edgar\n",
      "\n",
      "http://www.drive5.com/muscle\n",
      "This software is donated to the public domain.\n",
      "Please cite: Edgar, R.C. Nucleic Acids Res 32(5), 1792-97.\n",
      "\n",
      "00:03:09     43 MB(1%)  Iter   1  100.00%  Refine biparts\n",
      "00:06:48     43 MB(1%)  Iter   2  100.00%  Refine biparts\n",
      "00:09:20     43 MB(1%)  Iter   3  100.00%  Refine biparts\n",
      "00:11:26     43 MB(1%)  Iter   4  100.00%  Refine biparts\n",
      "00:11:26     43 MB(1%)  Iter   4  100.00%  Refine biparts\n"
     ]
    }
   ],
   "source": [
    "!muscle -in ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.hmmaligned.fasta -out ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta -refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the alignement if needs be \n",
    "## Save OSK and LOTUS domains as separate alignements \n",
    "- OSK: oskar_filtered.aligned.OSK_domain.fasta\n",
    "- LOTUS: oskar_filtered.aligned.LOTUS_domain.fasta\n",
    "## regenerate the OSK and LOTUS hmm and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.OSK_domain.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/OSK_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned.OSK_domain   380   664   187     4.10  0.590 \n",
      "\n",
      "# CPU time: 0.19u 0.01s 00:00:00.20 Elapsed: 00:00:00.26\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/OSK_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.OSK_domain.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.LOTUS_domain.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/LOTUS_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned.LOTUS_domain   380   166   101     7.40  0.590 \n",
      "\n",
      "# CPU time: 0.11u 0.01s 00:00:00.12 Elapsed: 00:00:00.13\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/LOTUS_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.LOTUS_domain.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
      "# HMMER 3.2.1 (June 2018); http://hmmer.org/\n",
      "# Copyright (C) 2018 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "# input alignment file:             ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta\n",
      "# output HMM file:                  ../Data/01_Oskar_identification/oskar_tracker_results/OSKAR_CONSENSUS.hmm\n",
      "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
      "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
      "1     oskar_filtered.aligned   380  2354  1024    16.97  0.590 \n",
      "\n",
      "# CPU time: 0.92u 0.02s 00:00:00.94 Elapsed: 00:00:01.02\n"
     ]
    }
   ],
   "source": [
    "!hmmbuild ../Data/01_Oskar_identification/oskar_tracker_results/OSKAR_CONSENSUS.hmm ../Data/01_Oskar_identification/oskar_tracker_results/oskar_filtered.aligned.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE V to the version you are working on\n",
    "!mkdir ../Data/Oskar_hmm/V3\n",
    "\n",
    "!cp ../Data/01_Oskar_identification/oskar_tracker_results/*.hmm ../Data/Oskar_hmm/V3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current version as search files\n",
    "!cp ../Data/Oskar_hmm/V3/*.hmm ../Data/Oskar_hmm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metadata table of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_type(x):\n",
    "    if 'GCA' in x:\n",
    "        return \"GCA\"\n",
    "    elif 'GCF' in x:\n",
    "        return 'GCF'\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def TSA_code(x):\n",
    "    return x[:4]\n",
    "\n",
    "def count_hits(x):\n",
    "    if x != \"None\":\n",
    "        return len(x.split(','))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def make_matcher(match, results):\n",
    "    mapping = {}\n",
    "    for ids, oskar in results[[match, 'oskar_hits']].values:\n",
    "        if oskar != \"None\":\n",
    "            for osk in oskar.split(','):\n",
    "                mapping[osk] = ids\n",
    "    return mapping\n",
    "\n",
    "def get_filtered_hits(fasta_path, mapping):\n",
    "    handle = SeqIO.parse(fasta_path, 'fasta')\n",
    "    sequences = [s for s in handle]\n",
    "    tmp = []\n",
    "    for s in sequences:\n",
    "        spl = s.description.split('|')\n",
    "        seq_id = spl[0].split(' ')[0]\n",
    "        specie = ' '.join(spl[0].split(' ')[1:])\n",
    "        family = spl[1]\n",
    "        order = spl[2]\n",
    "        source = spl[3]\n",
    "        source_id = mapping[seq_id]\n",
    "        tmp.append([source_id, seq_id, specie, family, order, source])\n",
    "    oskar_infos = pd.DataFrame(tmp, columns=['id', 'Sequence_id', 'Specie', 'Family', 'order_name', 'Source'])\n",
    "    return oskar_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_TSA = pd.read_csv(\"../Data/01_Oskar_identification/2019/transcriptome_insect_database.csv\")\n",
    "source_genome = pd.read_csv(\"../Data/01_Oskar_identification/2019/genome_insect_database.csv\")\n",
    "source_genome['source'] = source_genome['genome_id'].map(genome_type)\n",
    "source_TSA['id'] = source_TSA['tsa_id']\n",
    "source_TSA['source'] = \"TSA\"\n",
    "# source_TSA['tsa_code'] = source_TSA['tsa_id'].map(TSA_code)\n",
    "source_TSA = source_TSA[['id', 'tax_id', 'species', 'family_name', 'order_name', 'source']]\n",
    "source_genome['id'] = source_genome['genome_id']\n",
    "source_genome = source_genome[['id', 'tax_id', 'species', 'family_name', 'order_name', 'source']]\n",
    "ncbi_metadata = source_TSA.append(source_genome).reset_index()\n",
    "ncbi_metadata.to_csv('../Data/Tables/Table_S1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSA_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_results.csv')\n",
    "TSA_mapping = make_matcher('tsa_id', TSA_search_results)\n",
    "TSA_search_results['id'] = TSA_search_results['tsa_id']\n",
    "TSA_search_results['source'] = 'TSA'\n",
    "# TSA_search_results = TSA_search_results[TSA_search_results['oskar_hits'] != 'None']\n",
    "TSA_search_results['hits'] = TSA_search_results['oskar_hits'].map(count_hits)\n",
    "TSA_search_results = TSA_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "TSA_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/TSA/tsa_oskar_filtered.fasta', TSA_mapping)\n",
    "TSA_hits = TSA_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "TSA_hits['filtered_hits'] = TSA_hits['Specie']\n",
    "del(TSA_hits['Specie'])\n",
    "TSA_search_results = TSA_search_results.merge(TSA_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCF_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_results.csv')\n",
    "GCF_mapping = make_matcher('genome_id', GCF_search_results)\n",
    "GCF_search_results['id'] = GCF_search_results['genome_id']\n",
    "GCF_search_results['source'] = 'GCF'\n",
    "# GCF_search_results = GCF_search_results[GCF_search_results['oskar_hits'] != 'None']\n",
    "GCF_search_results['hits'] = GCF_search_results['oskar_hits'].map(count_hits)\n",
    "GCF_search_results = GCF_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "GCF_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/GCF/gcf_oskar_filtered.fasta', GCF_mapping)\n",
    "GCF_hits = GCF_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "GCF_hits['filtered_hits'] = GCF_hits['Specie']\n",
    "del(GCF_hits['Specie'])\n",
    "GCF_search_results = GCF_search_results.merge(GCF_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCA_search_results = pd.read_csv('../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_results.csv')\n",
    "GCA_mapping = make_matcher('genome_id', GCA_search_results)\n",
    "GCA_search_results['id'] = GCA_search_results['genome_id']\n",
    "GCA_search_results['source'] = 'GCA'\n",
    "# GCA_search_results = GCA_search_results[GCA_search_results['oskar_hits'] != 'None']\n",
    "GCA_search_results['hits'] = GCA_search_results['oskar_hits'].map(count_hits)\n",
    "GCA_search_results = GCA_search_results[['id', 'species', 'family_name', 'order_name', 'hits', 'source']]\n",
    "GCA_hits = get_filtered_hits('../Data/01_Oskar_identification/oskar_tracker_results/GCA/gca_oskar_filtered.fasta', GCA_mapping)\n",
    "GCA_hits = GCA_hits.groupby('id', as_index=False).count()[['id', 'Specie']]\n",
    "GCA_hits['filtered_hits'] = GCA_hits['Specie']\n",
    "del(GCA_hits['Specie'])\n",
    "GCA_search_results = GCA_search_results.merge(GCA_hits, on='id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_metadata = TSA_search_results.append(GCF_search_results).append(GCA_search_results).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_metadata.to_csv('../Data/01_Oskar_identification/oskar_tracker_results/search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
